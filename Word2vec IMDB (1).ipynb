{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><font size=5><font color='gray'>**Word2vec**</font></font></center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from collections import Counter\n",
    "from string import punctuation, ascii_lowercase\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vibhanshuv\\Downloads\\Case Study 1\\aclImdb\n"
     ]
    }
   ],
   "source": [
    "% cd \"C:\\Users\\vibhanshuv\\Downloads\\Case Study 1\\aclImdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('pre_train.csv',sep=\",\")\n",
    "test_df=pd.read_csv('pre_test.csv',sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the order of dataframes for better training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.reindex(np.random.permutation(train_df.index))\n",
    "test_df=test_df.reindex(np.random.permutation(test_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>description</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>words</th>\n",
       "      <th>words_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19844</th>\n",
       "      <td>19844</td>\n",
       "      <td>amongst the standard one liner type action fil...</td>\n",
       "      <td>1</td>\n",
       "      <td>['amongst', 'standard', 'one', 'liner', 'type'...</td>\n",
       "      <td>amongst standard one liner type action film ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>2479</td>\n",
       "      <td>i saw this movie, and at times, i was unnerved...</td>\n",
       "      <td>0</td>\n",
       "      <td>['saw', 'movie', 'time', 'unnerved', 'believin...</td>\n",
       "      <td>saw movie time unnerved believing movie 'saw m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19784</th>\n",
       "      <td>19784</td>\n",
       "      <td>saw this again recently on comedy central. i w...</td>\n",
       "      <td>1</td>\n",
       "      <td>['saw', 'again', 'recently', 'comedy', 'centra...</td>\n",
       "      <td>saw again recently comedy central would love s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22950</th>\n",
       "      <td>22950</td>\n",
       "      <td>after a big tip of the hat to spinal tap, this...</td>\n",
       "      <td>1</td>\n",
       "      <td>['big', 'tip', 'hat', 'spinal', 'tap', 'movie'...</td>\n",
       "      <td>big tip hat spinal tap movie hilarious anyone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11029</th>\n",
       "      <td>11029</td>\n",
       "      <td>after watching \"the bodyguard\" last night, i f...</td>\n",
       "      <td>0</td>\n",
       "      <td>['watching', '``', 'bodyguard', \"''\", 'last', ...</td>\n",
       "      <td>watching `` bodyguard '' last night felt compe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                        description  \\\n",
       "19844       19844  amongst the standard one liner type action fil...   \n",
       "2479         2479  i saw this movie, and at times, i was unnerved...   \n",
       "19784       19784  saw this again recently on comedy central. i w...   \n",
       "22950       22950  after a big tip of the hat to spinal tap, this...   \n",
       "11029       11029  after watching \"the bodyguard\" last night, i f...   \n",
       "\n",
       "       sentiment                                              words  \\\n",
       "19844          1  ['amongst', 'standard', 'one', 'liner', 'type'...   \n",
       "2479           0  ['saw', 'movie', 'time', 'unnerved', 'believin...   \n",
       "19784          1  ['saw', 'again', 'recently', 'comedy', 'centra...   \n",
       "22950          1  ['big', 'tip', 'hat', 'spinal', 'tap', 'movie'...   \n",
       "11029          0  ['watching', '``', 'bodyguard', \"''\", 'last', ...   \n",
       "\n",
       "                                              words_sent  \n",
       "19844  amongst standard one liner type action film ac...  \n",
       "2479   saw movie time unnerved believing movie 'saw m...  \n",
       "19784  saw again recently comedy central would love s...  \n",
       "22950  big tip hat spinal tap movie hilarious anyone ...  \n",
       "11029  watching `` bodyguard '' last night felt compe...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=train_df['sentiment']\n",
    "Y_test=test_df['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "vocab = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, lower=False):\n",
    "  \n",
    "    # Tokenize\n",
    "    text = tokenizer.tokenize(text)\n",
    "    \n",
    "    # optional: lower case\n",
    "    if lower:\n",
    "        text = [t.lower() for t in text]\n",
    "    \n",
    "    # Return a list of words\n",
    "    vocab.update(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_comments(list_sentences, lower=False):\n",
    "    comments = []\n",
    "    for text in tqdm(list_sentences):\n",
    "        txt = text_to_wordlist(text, lower=lower)\n",
    "        comments.append(txt)\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = list(train_df[\"words_sent\"].values)\n",
    "list_sentences_test = list(test_df[\"words_sent\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 50000/50000 [00:05<00:00, 9940.90it/s]\n"
     ]
    }
   ],
   "source": [
    "comments = process_comments(list_sentences_train + list_sentences_test, lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary contains 98201 unique tokens\n"
     ]
    }
   ],
   "source": [
    "print(\"The vocabulary contains {} unique tokens\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(comments, window=5, size=5000,min_count=5, workers=16, sg=0, negative=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_preprocessing(df):\n",
    "    \"\"\" All the preprocessing steps for word2vec are done in this function.\n",
    "    All mutations are done on the dataframe itself. So this function returns\n",
    "    nothing.\n",
    "    \"\"\"\n",
    "    df['text'] = df.description.str.lower()\n",
    "    df['document_sentences'] = df.text.str.split('.')  # split texts into individual sentences\n",
    "    df['tokenized_sentences'] = list(map(lambda sentences:\n",
    "                                         list(map(nltk.word_tokenize, sentences)),\n",
    "                                         df.document_sentences))  # tokenize sentences\n",
    "    df['tokenized_sentences'] = list(map(lambda sentences:\n",
    "                                         list(filter(lambda lst: lst, sentences)),\n",
    "                                         df.tokenized_sentences))  # remove empty lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_preprocessing(train_df)\n",
    "\n",
    "w2v_preprocessing(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_features(w2v_model, sentence_group):\n",
    "    \"\"\" Transform a sentence_group (containing multiple lists\n",
    "    of words) into a feature vector. It averages out all the\n",
    "    word vectors of the sentence_group.\n",
    "    \"\"\"\n",
    "    words = np.concatenate(sentence_group)  # words in text\n",
    "    index2word_set = set(w2v_model.wv.vocab.keys())  # words known to model\n",
    "    \n",
    "    featureVec = np.zeros(w2v_model.vector_size, dtype=\"float32\")\n",
    "    \n",
    "    # Initialize a counter for number of words in a review\n",
    "    nwords = 0\n",
    "    # Loop over each word in the comment and, if it is in the model's vocabulary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            featureVec = np.add(featureVec, w2v_model[word])\n",
    "            nwords += 1.\n",
    "\n",
    "    # Divide the result by the number of words to get the average\n",
    "    if nwords > 0:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vibhanshuv\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "train_df['w2v_features'] = list(map(lambda sen_group:\n",
    "                                      get_w2v_features(model, sen_group),\n",
    "                                      train_df.tokenized_sentences))\n",
    "\n",
    "test_df['w2v_features'] = list(map(lambda sen_group:\n",
    "                                      get_w2v_features(model, sen_group),\n",
    "                                      test_df.tokenized_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v = np.array(list(map(np.array, train_df.w2v_features)))    \n",
    "\n",
    "X_test_w2v = np.array(list(map(np.array, test_df.w2v_features)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86528"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0)\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "#parameters={'C':[1.0,10.0,12.0,20,50,100]}\n",
    "#clf=GridSearchCV(model,param_grid=parameters,cv=3)\n",
    "clf.fit(X_train_w2v,Y_train)\n",
    "#clf.best_params_\n",
    "#print('best score :{0:.2F}'.format(clf.best_score_))\n",
    "clf.score(X_test_w2v,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84616"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X_train_w2v,Y_train)\n",
    "model.score(X_test_w2v,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score :0.87\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.87256"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(random_state=0)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters={'C':[1.0,10.0,12.0,20,50,100]}\n",
    "clf=GridSearchCV(model,param_grid=parameters,cv=3)\n",
    "clf.fit(X_train_w2v,Y_train)\n",
    "clf.best_params_\n",
    "print('best score :{0:.2F}'.format(clf.best_score_))\n",
    "clf.score(X_test_w2v,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83592"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model = AdaBoostClassifier(n_estimators=100)\n",
    "model.fit(X_train_w2v,Y_train)\n",
    "model.score(X_test_w2v,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71016"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "classifier=BernoulliNB().fit(X_train_w2v,Y_train)\n",
    "classifier.score(X_test_w2v,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86332"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier(max_iter=1000)\n",
    "clf.fit(X_train_w2v,Y_train)\n",
    "clf.score(X_test_w2v,Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
